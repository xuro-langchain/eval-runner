version: apiv1alpha1.evaluation.infra
name: evaluation_metrics
kind: LangSmithEvaluation
metadata:
  # TBD.
spec:
  type: RunExperimentByDataset
  dataset: cs_intent_eng_query
  metrics:
    - type: LLMAsJudge
      category: BuiltinMetric
      name: Correctness
      model:
        provider: openai
        name: gpt-4o
        temperature: 0.5
    - type: LLMAsJudge
      category: CustomMetric 
      name: Correctness # will be used as feedback key
      model:
        provider: openai
        name: gpt-4o
        temperature: 0.5
        # any field supported on the Chat model can be added here
      prompt:
        - role: system
          content: |
            ...
        - role: human
          content: |
            ...
      feedback_configuration:
          type: score
          use_reasoning: True
    - type: Code
      category: BuiltinMetric
      name: ExactMatch
      input: input.user_query
      expect_output: expect_output
      output: output
  evaluate:
    target_type: DifyApplication
    target:
      url: https://dify-internal.corp
      app: ${{ secrets.DIFY_APP_ID }}
      api_key: ${{ secrets.DIFY_APP_KEY }}
    input: input.user_query
    output: output
